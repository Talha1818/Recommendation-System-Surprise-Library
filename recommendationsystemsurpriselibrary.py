# -*- coding: utf-8 -*-
"""RecommendationSystemSurpriseLibrary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UDFZODIzgzWCPbmUWt5NSEfYgrrwVSOE
"""

# improt the necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
from tabulate import tabulate


import warnings
warnings.filterwarnings('ignore')

df1 = pd.read_csv("phone_user_review_file_1.csv",encoding='latin-1')
df2 = pd.read_csv("phone_user_review_file_2.csv",encoding='latin-1')
df3 = pd.read_csv("phone_user_review_file_3.csv",encoding='latin-1')
df4 = pd.read_csv("phone_user_review_file_4.csv",encoding='latin-1')
df5 = pd.read_csv("phone_user_review_file_5.csv",encoding='latin-1')
df6 = pd.read_csv("phone_user_review_file_6.csv",encoding='latin-1')

# 1. Merge all the provided CSVs into one dataFrame.  
df = pd.concat([df1,df2,df3,df4,df5,df6], axis= 0)

# 2. Explore, understand the Data and share at least 2 observations.  
print("Rows:", df.shape[0])
print("Columns:", df.shape[1])

df.head()

# lets try to checl unique users and product
print("Unique Users: ",df.author.nunique())
print("Unique Products: ",df['product'].nunique())

# Top users, rated good on average
data_user_rate=pd.DataFrame(df.groupby('author')['score'].agg(['mean','count']),
                            columns=['mean','count']).sort_values(by=['count','mean'],ascending=False)[:20]
data_product_rate=pd.DataFrame(df.groupby('product')['score'].agg(['mean','count']),
                            columns=['mean','count']).sort_values(by=['count','mean'],ascending=False)[:20]
print("*"*100)
print(data_user_rate.head(5))
print("*"*100)
print(data_product_rate.head(5))
print("*"*100)

def pie_chart(df, col, top=None):
    # lets try to check the distributin of specific column
    df[col].value_counts()[:top].plot(kind='pie', figsize=(15,8),autopct='%1.0f%%',textprops={'fontsize': 14})
    plt.title(f"All {col} Distribution in Dataset", fontsize=18)
    plt.show()

pie_chart(df, 'lang')
pie_chart(df, 'country')
pie_chart(df, 'source', top=5)
pie_chart(df, 'domain', top=5)
pie_chart(df, 'product', top=5)

"""* From above analysis, we can see the distribution of features.
* english language (39%) is dominent as compare to other langs in e-commerce.
* Most envolve the US contry (23%) in e-commerce field.
* Most selling/buying smarphone/electronic through Amazon (74%).
* Domain amazon.com (32%) is most use as compare to other amazon domain.
* Lenovo Vibe K4 Note (White,16GB) is a demand product.
"""

print(df[['score','score_max']].describe().T)
df.select_dtypes(include=[int, float]).hist()
plt.show()

# 3. Round off scores to the nearest integers.  
numeirc_cols = df.select_dtypes(include=[int, float]).columns.tolist()
for col in numeirc_cols:
    df[col] = round(df[col], 2)

# 4. Check for missing values. Impute the missing values, if any.  
df.isnull().sum()

# lets fill the miussing values of categorical features with most frequent values and
# numerical features with mean/average
for i in list(df.columns):
    if df[i].dtype == 'object':
        df[i] = df[i].fillna(df[i].mode().values[0])
    else:
        df[i] = round(df[i].fillna(df[i].mean()),2)

df.isnull().sum()

# 5. Check for duplicate values and remove them, if any
bool_series = df.duplicated(keep='first')
df = df[~bool_series]

df.head()

# 6. Keep only 1 Million data samples. Use random state=612.  
df = df.sample(n=1000000, random_state=612).reset_index(drop=True)
print(df.shape)

# 7. Drop irrelevant features. Keep features like Author, Product, and Score.  
df = df[['author', 'product', 'score']]
df.head()

# 1. Identify the most rated products. 
top5Products = df.groupby('product')['score'].count().sort_values(ascending=False)[:5]
top5Products.plot(kind='bar', color=['Red', 'Green'])
plt.show()
print(top5Products)

"""* From above analysis, we can see that Lenovo Vibe K4 Note (White,16GB) is a most rated product as compare to other products."""

# 2. Identify the users with most number of reviews.  
top5Users = df.groupby('author')['score'].count().sort_values(ascending=False)[:5]
top5Users.plot(kind='bar', color=['Red', 'Green'])
plt.show()
print(top5Users)

"""* From above analysis, we can see that Amazon users is the most reviews as compare to other users."""

# 3. Select the data with products having more than 50 ratings and users who have given more than 50 ratings. 
# Report the shape of the final dataset
author_df = df.author.value_counts()
product_df = df['product'].value_counts()
data_final=df[df['author'].isin(author_df[author_df>=50].index) & df['product'].isin(product_df[product_df>=50].index)]
print("Shape of Dataset:", data_final.shape)

# Count of product for each unique user as recommendation score 
data_grouped = data_final.groupby('product').agg({'author':'count','score':'mean'}).reset_index()
data_grouped.rename(columns={'author':'count','score':'mean'},inplace=True)

# Sort the product on recommendation score 
data_sort = data_grouped.sort_values(['count'], ascending = False) 
      
# Generate a recommendation rank based upon score 
data_sort['Rank'] = data_sort['count'].rank(ascending=False, method='first') 
          
# Get the top 5 recommendations 
popularity_recommendations = data_sort.reset_index(drop=True).set_index('product').head() 
popularity_recommendations

# 4. Build a collaborative filtering model using SVD. You can use SVD from surprise or build it from scratch
# (Note: Incase youâ€™re building it from scratch you can limit your data points to 5000 samples if you face memory issues).
# Build a collaborative filtering model using kNNWithMeans from surprise. You can try both user-based and item-based model.

# conda install -c conda-forge scikit-surprise

from surprise import Reader
from surprise import Dataset

from surprise.model_selection import cross_validate
from surprise import KNNWithMeans
from surprise import SVD

from surprise.accuracy import rmse
from surprise.model_selection import train_test_split

reader = Reader(rating_scale=(1, 50))
data = Dataset.load_from_df(data_final[['author', 'product', 'score']], reader)

# test set is made of 30% of the ratings.
trainset, testset = train_test_split(data, test_size=0.3)

# SVD
svd = SVD()
svd.fit(trainset)

# kNNWithMeans 
knn = KNNWithMeans()
knn.fit(trainset)

# predict test users from svd
predictions = svd.test(testset)
rmse_svd = rmse(predictions)

# predict test users from knn
predictions_knn = knn.test(testset)
rmse_knn = rmse(predictions_knn)

rmse_ = pd.DataFrame([rmse_svd,rmse_knn], index = ['SVD','KNNWithMeans'], columns=['RMSE'])
rmse_

# get a prediction for specific users and items.
user = 'Amazon Customer'
product = 'OnePlus X (Onyx, 16GB)'
pred = svd.predict(user, product , verbose=False)
print(f" \n\tuser:{user}\n\tproduct:{product}\n\tRating:{round(pred.est,2)}")

# 6. Predict score (average rating) for test users.  

# Prediction SVD model
average_rat = pd.DataFrame(predictions)[['iid','r_ui','est']].rename(columns={'iid':'product', 'r_ui':'actual_score','est':'predicted_score'})
average_rat = average_rat.groupby('product')['actual_score','predicted_score'].mean().reset_index()
average_rat['avg_rating'] = average_rat['predicted_score']/average_rat.shape[0]
average_rat.sort_values(by=['predicted_score','avg_rating'], ascending=[0,0])

# Prediction KNN model
average_rat_knn = pd.DataFrame(predictions_knn)[['iid','r_ui','est']].rename(columns={'iid':'product', 'r_ui':'actual_score','est':'predicted_score'})
average_rat_knn = average_rat_knn.groupby('product')['actual_score','predicted_score'].mean().reset_index()
average_rat_knn['avg_rating'] = average_rat_knn['predicted_score']/average_rat_knn.shape[0]
average_rat_knn.sort_values(by=['predicted_score','avg_rating'], ascending=[0,0])

"""7. Report your findings and inferences
* From above analysis, we fit the SVD and KnnwithMeans and predicted the rating against product in which predicted average rating of product `Huawei Honor Pro` is a most recommended product using SVD model.
* KnnwithMeans recommended the best product that has highest predicted average rating is `Microsoft Nokia 1650 dark red (Farbdisplay, UKW-Stereo-Radio, Organizer, Spiele) Handy`.
* SVD (0.002634) best product average rating is less than KNNwithMeans model (0.002927).
* Therefore, `Microsoft Nokia 1650 dark red (Farbdisplay, UKW-Stereo-Radio, Organizer, Spiele) Handy` is most preferable as compare to `Huawei Honor Pro
"""

# 8.Try and recommend top 5 products for test users.  

# SVD Recommended Products
print("\t\tSVD Recommended Products")
topRecommProductSVD = average_rat.sort_values(by=['avg_rating'], ascending=[0])
topRecommProductSVD['Rank'] = topRecommProductSVD['avg_rating'].rank(ascending=False, method='first')
topRecommProductSVD.reset_index(drop=True).set_index('product').head()

# KNN Recommended Products
print("\t\tKNN Recommended Products")
topRecommProductKNN = average_rat_knn.sort_values(by=['avg_rating'], ascending=[0])
topRecommProductKNN['Rank'] = topRecommProductKNN['avg_rating'].rank(ascending=False, method='first')
topRecommProductKNN.reset_index(drop=True).set_index('product').head()

# 9. Try other techniques (Example: cross validation) to get better results.  

# Run 5-fold cross-validation and print results SVD
svd_cross_validate_res = cross_validate(svd, data, measures=['RMSE'], cv=5, verbose=True)

print("SVD Cross Validation Results\n")
print(pd.DataFrame(svd_cross_validate_res).mean(axis = 0))

# Run 5-fold cross-validation and print results KNNwithMeans
knn_cross_validate_res = cross_validate(knn, data, measures=['RMSE'], cv=5, verbose=True)

print("KNNWithMeans Cross Validation Results\n")
print(pd.DataFrame(knn_cross_validate_res).mean(axis = 0))

# 10. In what business scenario you should use popularity based Recommendation Systems ?

"""* Popularity based recommendation system works with the trend. It basically uses the items which are in trend right now. For example, if any product which is usually bought by every new user then there are chances that it may suggest that item to the user who just signed up.
* The Popularity-based recommender system is non-personalised and the recommendations are based on frequecy counts, which may be not suitable to the user
"""

# 11. In what business scenario you should use CF based Recommendation Systems ?

"""* Most websites like Amazon, YouTube, and Netflix use collaborative filtering as a part of their sophisticated recommendation systems. You can use this technique to build recommenders that give suggestions to a user on the basis of the likes and dislikes of similar users
* Model-based Collaborative Filtering is a personalised recommender system, the recommendations are based on the past behavior of the user and it is not dependent on any additional information.
* Collaborative Filtering based model has recommended entire different list based on the user past purchase history
"""

# 12. What other possible methods can you think of which can further improve the recommendation for different users ?

"""We can Improve the recommendation
* Ditch Your User-Based Collaborative Filtering Model
* A Gold Standard Similarity Computation Technique
* Boost Your Algorithm Using Model Size
"""